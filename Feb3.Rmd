---
title: "2nd Preliminary analysis for 2019-nCoV cases reported in some Asian countries and regions"
author: "Qingyuan Zhao"
date: "Febraury 3, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The 2nd preliminary analysis uses almost the same dataset as the [first preliminary analysis](https://htmlpreview.github.io/?https://github.com/qingyuanzhao/2019-nCov-Data/blob/master/Feb1.html) but attempts to use a simple model to acknowledge that this dataset contains only "shadows" of the real epidemic in Wuhan. Another important distinction is that we will directly model the infection time that can be imputed by the symptom onset time and the incubation interval reported in [Li et al. (2020)](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316).

# Data preprocessing

We first read and pre-process the data. This is very similar to the [first preliminary analysis](https://htmlpreview.github.io/?https://github.com/qingyuanzhao/2019-nCov-Data/blob/master/Feb1.html). The only difference is that we now start our date indexing from the December 1st instead of January 1st (because some infections happened in December).
```{r}
source("functions.R")

data <- read.table("Feb2.tsv", sep = "\t", header = TRUE)

data$Confirmed <- date.process(data$Confirmed)
data$Arrived <- date.process(data$Arrived)
data$Symptom <- date.process(data$Symptom)
data$Initial <- date.process(data$Initial)
data$Hospital <- date.process(data$Hospital)

## Only focus on the following countries/regions: Japan, Singapore, Taiwan, HongKong, Macau, Korea
data$Country_or_Region <- do.call(rbind, strsplit(as.character(data$Case), "-"))[, 1]
data <- subset(data, Country_or_Region %in% c("Japan", "Singapore", "Taiwan", "Korea", "Hong Kong", "Macau"))
table(data$Country_or_Region)

## Only consider cases who were (most certainly) infected in Wuhan and arrived on or before 23th of January
data <- subset(data, Outside != "Y" & Arrived <= 23+31)
nrow(data)
```

Because of the lockdown of Wuhan on January 23rd, the infection time can no longer be later than January 23rd, which is day
```{r}
(N <- 31 + 23)
```
in our series.

# Imputation of infection time

The first novelty of this analysis is that we will use existing information about the infection date of the cases. Occasionally, it may be possible to narrow down the infection to one or several days. For example, the 12th confirmed case in Japan only stayed in Wuhan stayed during January 16--22 according to this [official report](https://www.mhlw.go.jp/stf/newpage_09239.html). We also know that the infection date ought to be no later than the arrival date. The *parse.infect* function creates two columns, /Infected_first/ and /Infected_last/, that contain such information
```{r}
data <- parse.infected(data)
subset(data, Case == "Japan-12")
```

To illustrate the imputation of infection time, we first impute the few missing symptom onset dates using the *simple.impute.onset* function (described in the [first preliminary analysis](https://htmlpreview.github.io/?https://github.com/qingyuanzhao/2019-nCov-Data/blob/master/Feb1.html)).
```{r}
## Simple imputation of symptom onset
set.seed(20200202)
symptom.imputed <- simple.impute.onset(data)
```

The infection date is imputed by the symptom onset date minus a random draw from the distribution of the incubation period, truncated to the infection interval. This is implemented in the *impute.infected* function. Notice that [Li et al. (2020)](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316) only reported the estimated mean (5.2 days) and 95% quantile (12.5 days) of the incubation period. I matched them with a gamma distribution, although the histogram is slightly different from Figure 2A in that article.
```{r}
infected.imputed <- impute.infected(symptom.imputed, data$Infected_first, data$Infected_last,
                                    incubation_alpha = 1.92, incubation_beta = 0.37) 
```

We can visualize the imputed infection time by counting the incidences on each day:
```{r}
as.count <- function(infected, last_date = 23+31) {
    table(factor(infected, levels = 1:last_date))
}
plot(as.count(infected.imputed))
```

# Distribution of the infection time

We can obtain a distribution of the infection time for these individuals by repeating the above procedure. To avoid just using one imputation of the sympton onset, we use the *mice* package to impute it using the arrival, initial medical visit, hospitalization, and confirmation dates (see [first preliminary analysis](https://htmlpreview.github.io/?https://github.com/qingyuanzhao/2019-nCov-Data/blob/master/Feb1.html)).

```{r, message = FALSE}
set.seed(20200202)
m <- 1000
symptom_imputed <- multiple.impute.onset(data, m = m)
```

```{r}
infected_imputed <- matrix(0, nrow(data), m)
OI_imputed <- matrix(0, N, m)
for (impute in 1:m) {
    infected_imputed[, impute] <- impute.infected(symptom_imputed[, impute], 
                                                  data$Infected_first, data$Infected_last)
    OI_imputed[, impute] <- as.count(infected_imputed[, impute])
}
```

```{r}
library(reshape2)
library(ggplot2)
df <- melt(OI_imputed[, 1:100])
names(df) <- c("date", "impute", "count")
df$date <- df$date - 1 + as.Date("2019-12-01")
ggplot(df) + aes(x = date, y = count) +
  geom_point(data = df, position = position_jitter(width = 0.5), alpha = 0.2) + 
  geom_smooth(method = "loess", span = 0.1)
```

We see that the infection counts were initially growing exponentially but dropped in the last few days. This phenomenon may look surprising in the beginning, but actually this is due to we are using a sample that left Wuhan before the lockdown. For people infected on January 22, there was not enough time for all of them to leave. Due to the nature of our sample, we posit the following model. Let $WI_t$ be the number of new infections in Wuhan on day $t$, among which $OI_t$ left Wuhan on or before January 23. We assume $WI_t$ was growing exponentially before January 23:
\begin{equation} \label{eq:wi}
  WI_t = WI_0 \cdot e^{rt}
\end{equation}
and $OI_t$ follows a Poisson distribution:
\begin{equation} \label{eq:oi}
  OI_t \sim \text{Poisson}(OP * (N - t + 1) * WI_t),
\end{equation}
where the $OP$ is a constant representing the proportion of people leaving Wuhan. An immediate consequence of this model is that the logarithm of the expectation of $OI_t$ is given by
\begin{equation} \label{eq:log-linear}
  \log\big(\mathbb{E}[OI_t]\big) = rt + \log(N - t + 1) + \text{constant}.
\end{equation}
We can verify this model by estimating the left hand side using random samples of $(OI_t)_{t=1}^N$:
```{r}
df <- data.frame(date = 1:N, OI_mean = apply(OI_imputed, 1, mean))
plot(log(OI_mean / (N - date + 1)) ~ date, df)
fit <- lm(log(OI_mean / (N - date + 1)) ~ date, subset(df, date > 15 & date < 50))
abline(fit$coefficients[1], fit$coefficients[2], col = "red")
```

The slope of this simple straight line fit, which estimates $r$ according to the last display, is given by
```{r}
(r <- fit$coef[2])
```

This estimate means that the epidemic was doubling every
```{r}
log(2) / r
```
days. We can use the equation (3.4) in [Wallinga and Lipsitch (2006)](https://royalsocietypublishing.org/doi/full/10.1098/rspb.2006.3754) to estimate the basic reproduction number from $r$. This formula assumes the serial interval is normally distributed, and we will use the mean 7.5 and standard deviation 3.4 reported by [Li et al. (2020)](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316).
```{r}
r.to.R <- function(r, si_mean = 7.5, si_sd = 3.4) {
    exp(r * si_mean - r^2 * si_sd^2 / 2)
}
r.to.R(r)
```
This is much higher than the $R_0$ estimated by [Li et al. (2020)](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316).

# Uncertainty quantification

The above analysis does not recognize the fact that the distribution of $(OI_t)_{t=1}^N$ is drawn based on the observed cases, which is only a small (hopefully random) sample of the cases in Wuhan. We use Bayesian inference to quantify the sampling uncertainty. For each draw of $(OI_t)_{t=1}^n$, we fit the above model with the following prior:
\begin{align*}
  r &\sim \text{Exponential}(\text{mean} = \log(2)/7.4),\\
  WI_{32} &\sim \text{Gamma}(\text{mean} = 50, \text{sd} = 100),
  OP \sim \text{Exponential}(OP_\text{prior}).
\end{align*}
The prior mean of $r$ is chosen as the estimate from [Li et al. (2020)](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316) who estimated the cases were doubly every $7.4$ days. We choose to put a diffuse Gamma prior on $WI_{32}$, the number of new infections on January 31st, which is more interpretable than $WI_1$. 
```{r}
stan_data <- list(N = N,
                  r_prior_mean = log(2) / 7.4,
                  WI_Jan1_prior_mean = 50,
                  WI_Jan1_prior_sd = 100)
```
Finally we put an exponential prior on $OP$, the proportion of people leaving from Wuhan to the selected Asian countries/regions. We estimate the travel to the selected Asian countries/regions using air traffic planning data from December 30, 2019 to January 22, 2020, reported by this [web article](https://www.jiqizhixin.com/articles/2020-01-27-2). We assume twice as many people entered Hong Kong and Macau via train/car/ferry than air. We assume 80\% of the planned aircraft seats were taken.
```{r}
daily_travel <- (7078 * 3 + # Hong Kong
                  6154 * 3 + # Macau
                  3696 + 2698 + 1121 + # Taiwan
                  10680 + # Singapore
                  9080 + 6272 + 2656 + # Japan
                  6430) / 24 * 0.8  # Korea
daily_travel
stan_data$OP_prior_mean <- daily_travel / 11000000 ## divide by Wuhan's population
```

The next code chunk implements this Bayesian model in *stan*:
```{r, cache = TRUE, message = FALSE}
stan_code <- "
data {
  int<lower=0> N;
  int<lower=0> OI[N];
  real<lower=0> WI_Jan1_prior_mean;
  real<lower=0> WI_Jan1_prior_sd;
  real<lower=0> OP_prior_mean;
  real<lower=0> r_prior_mean;
}
parameters {
  real<lower=0> r;
  real<lower=0> WI_Jan1;
  real<lower=0> OP;
}
transformed parameters {
  vector[N] WI;
  vector[N] OI_mean;
  for (t in 1:N) {
    WI[t] = WI_Jan1 * exp(r * (t - 32));
    OI_mean[t] = (N - t + 1) * OP * WI[t];
  }
}
model {
  r ~ exponential(1 / r_prior_mean);
  WI_Jan1 ~ gamma(WI_Jan1_prior_mean^2 / WI_Jan1_prior_sd^2,
                  WI_Jan1_prior_mean / WI_Jan1_prior_sd^2);
  OP ~ exponential(1 / OP_prior_mean);
  OI ~ poisson(OI_mean);
}
generated quantities {
  vector[N] WT;
  WT = cumulative_sum(WI);
}
"

library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
sm <- stan_model(model_code = stan_code)
```

We draw 50 samples of $(OI)_{t=1}^N$ and obtain posterior samples of $r$, $WI_{32}$, and $AP$ in this model:
```{r, message = FALSE, cache = TRUE, warning = FALSE, message = "hide"}
set.seed(20200203)

m <- 50
symptom_imputed <- multiple.impute.onset(data, m = m)

set.seed(2020020319)
posterior <- list()
for (impute in 1:m) {
    infected_imputed <- impute.infected(symptom_imputed[, impute],
                                        data$Infected_first, data$Infected_last)
    stan_data$OI <- as.count(infected_imputed, N)
    capture.output(fit <- sampling(sm, data  = stan_data, init = 3, iter = 20000, thin = 100),
                   file = "NUL") # supress the output of stan
    if (impute %% 10 == 0)
      print(fit, pars = "r")
    posterior[[impute]] <- extract(fit)
}
```

```{r, echo = FALSE}
save(posterior, file = "Feb3_posterior.rda")
```

The posterior mean and 95% credible interval of $r$ is
```{r}
r_posterior <- unlist(lapply(posterior, function(posterior) posterior$r))
my.summary <- function(x) {c(mean = mean(x), CI.low = quantile(x, 0.025), CI.up = quantile(x,0.975))}
my.summary(r_posterior)
my.summary(log(2) / r_posterior) ## doubling days
my.summary(r.to.R(r_posterior)) ## R0
```

# Sensitivity analysis

In the *stan* code we have also generated the total number of infections in Wuhan by the end of January 23. 

```{r}
WT_Jan23_posterior <- 
  unlist(lapply(posterior, function(posterior) posterior$WT[, N]))
my.summary(WT_Jan23_posterior)
```

However, this is not very reliable because it is closely correlated with the assumed rate of international traveling. People who traveled internationally are also more likely to be living in the city center and might have higher chances of infection.

Nevertheless, the growth exponent $r$ should be relatively insensitive to the choice. This is illustrated below where we assume the the prior mean of $OP$ is five times as before.

```{r, message = FALSE, cache = TRUE, warning = FALSE, message = "hide"}
stan_data$OP_prior_mean <- stan_data$OP_prior_mean * 5

set.seed(2020020319)
posterior2 <- list()
for (impute in 1:m) {
    infected_imputed <- impute.infected(symptom_imputed[, impute],
                                        data$Infected_first, data$Infected_last)
    stan_data$OI <- as.count(infected_imputed, N)
    capture.output(fit2 <- sampling(sm, data  = stan_data, init = 3, iter = 20000, thin = 100),
                   file = "NUL") # supress the output of stan
    if (impute %% 10 == 0)
      print(fit, pars = "r")
    posterior2[[impute]] <- extract(fit2)
}
```

```{r}
r_posterior2 <- unlist(lapply(posterior2, function(posterior) posterior$r))
my.summary(r_posterior2)
WT_Jan23_posterior2 <- 
  unlist(lapply(posterior2, function(posterior) posterior$WT[, N]))
my.summary(WT_Jan23_posterior2)
```